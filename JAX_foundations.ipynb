{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mICnTh-Ttog"
      },
      "source": [
        "## Introduction to JAX\n",
        "    \n",
        "<img  align=\"center\" src=\"https://lh4.googleusercontent.com/EMOZuy32KcQiVoOixIdjlScxYzTO_gS9Sq2dtJMX8VpFHbk2RLkB5ZexQ6ebanoOIY8kNML_hJK6RBMo2w0Ox9KYYa8-gaLI_R2KXkSII39OW1nNsV91p62kVe-7Tzege7ic4LBA\" alt=\"JAX_logo\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuZd2XbTC4of"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rastringer/jax_notebooks/blob/master/JAX_foundations.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okHL0q2fTpav"
      },
      "source": [
        "JAX is a framework that enables high-performance numerical computing by merging Autograd and XLA (Accelerated Linear Algebra). Autograd was originally a library created for automatic differentiation of Python and NumPy code, which has since been added to JAX. XLA is an optimizing compiler for machine learning, which can significantly speed up workloads on both TPU and GPU devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThT9B3GoTpde"
      },
      "source": [
        "### Why learn JAX?\n",
        "\n",
        "#### High performance\n",
        "\n",
        "JAX has excellent support for accelerators such as GPUs and TPUs and leverages both XLA and Just-In-Time compilation for speedy numerical computation.\n",
        "\n",
        "#### Automatic differentiation\n",
        "\n",
        "JAX's grad transform function renders trivial the calculation of  gradients of complex functions for gradient descent, backpropagation and other optimization algorithms.\n",
        "Research and experimentation:\n",
        "The framework's flexibility grants low-level programming power to developers for effective and rapid prototyping and research. The ease with which JAX code can be run on any device, functional programming and dynamic computation graphs are ideal for experimentation with different machine learning models.\n",
        "\n",
        "#### Composable and functional\n",
        "\n",
        "JAX encourages a functional programming style, enabling clean and modular code. Its functions are pure, which means they produce the same output from the same input every time, limiting side effects and improving safety and resusibility.\n",
        "Plays well with others:\n",
        "JAX features interoperability with NumPy, and can be used in conjunction with TensorFlow and PyTorch. Users often combine the features of other libraries with the performance benefits of JAX.\n",
        "\n",
        "#### How does JAX compare to other frameworks?\n",
        "\n",
        "#### TensorFlow\n",
        "\n",
        "TensorFlow creates static computational graphs that define operations and dependencies before execution, enabling efficient optimization and deployment across devices.\n",
        "The tf.Gradient.Tape API supports automatic differentiation, and extensive support for TPUs and GPUs.\n",
        "The framework has a large and mature ecosystem and strong industry adoption. High-level APIs such as Keras and tf.Module make development easier, and TensorFlow Hub offers a repository of pre-trained models.\n",
        "\n",
        "#### PyTorch\n",
        "\n",
        "Uses \"eager\" execution, dynamically building computation graphs as operations are invoked. This flexibility can allow for more intuitive experimentation and debugging.\n",
        "The torch.autograd module enables automatic differentiation and computing gradients during the backward pass. The framework has good support for TPUs via the XLA compiler, and torch.cuda for GPU memory  management.\n",
        "PyTorch has a growing ecosystem focused on research and flexibility. Many state-of-the-art models are implemented and shared first using the framework.\n",
        "Its high-level API simplifies building neural networks and includes modules for optimizztion, data handling and visualization.\n",
        "\n",
        "#### JAX\n",
        "\n",
        "JAX combines elements of static and dynamic compilation, providing a hybrid approach in which code can be executed \"just-in-time\", or traced into static compilation graphs. This enables efficient execution and optimization while retaining flexibility and ease of debugging.\n",
        "The framework offers automatic differentiation through its functional programming model, leveraging function transformations to compute gradients and allow fine-grained control over differentiation.\n",
        "\n",
        "JAX enjoys excellent GPU and TPU support, integrating tightly with XLA and requiring zero code changes to switch between devices. Spreading data and computation across cores is made simple via its function transformations such as pmap.\n",
        "JAX has a smaller and rapidly-growing community, with popularity among researchers such as Google's DeepMind. It's lower-level API is intuitive to those already familiar with NumPy, and neural network libraries Flax and Haiku provide modules and optimizers for training models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYrLzYxT_aY"
      },
      "source": [
        "### Key concepts\n",
        "\n",
        "JAX provides an API similar to NumPy that is intuitive and familar to many researchers and engineers.\n",
        "The framework includes composable function transformations for just-in-time compilation, batching, automatic differentiation and parallelization.\n",
        "JAX can be run on TPU, GPU and CPU, without any code changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-79TEdNfUAsw"
      },
      "source": [
        "### Accelerated NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7riWQFMOSXul"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import random\n",
        "from jax import device_put\n",
        "from jax import grad, vmap, pmap, jit, make_jaxpr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ6gYOThomb-",
        "outputId": "d4646585-7d96-4b9d-f715-10b5f587f280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "x = jnp.arange(10)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QVgidlWdVYh"
      },
      "source": [
        "We can move this array from CPU to GPU or TPU.\n",
        "\n",
        "### A word on Colab TPUs\n",
        "\n",
        "It used to be easy to switch from GPU to TPU in Colabs, however the TPUs set up is now behind JAX version >=0.4, which requires TPU VMs (on GCP).\n",
        "\n",
        "JAX 0.4 and newer requires TPU VMs, which Colab does not provide at this time. You can still use jax 0.3.25 on Colab TPU, which is the version that comes installed by default on Colab TPU runtimes. If you've already updated JAX, you can choose Runtime->Disconnect and Delete Runtime to get a fresh TPU VM, and then skip the pip install step so that you keep the default jax/jaxlib version 0.3.25.\n",
        "\n",
        "For now, we will proceed with GPUs and run code on Cloud TPU VMs later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WZjQissH7Bks",
        "outputId": "5e5a17c1-f134-4de9-97a5-6ce0bda9b0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "print(jax.device_count())\n",
        "device_type = jax.devices()[0].device_kind\n",
        "device_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_TyQDZE7j0t"
      },
      "source": [
        "More on how JAX creates random numbers later. For now, let's initialize a pseudo random number generator (PRNG) key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o3Vxtz1D7h9u"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InPhZlAL70MI",
        "outputId": "d06bf785-3f3f-4d57-ee4e-c1fd61730ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is of shape: (1000, 1000)\n",
            "x has dtype: float32\n"
          ]
        }
      ],
      "source": [
        "key, subkey = random.split(key)\n",
        "x = random.normal(key, (1000, 1000))\n",
        "\n",
        "print(f\"x is of shape: {x.shape}\")\n",
        "print(f\"x has dtype: {x.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsKc93D07Oe-",
        "outputId": "e4da9fc3-03b0-4c85-d4e8-f0284a55b4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array(x)\n",
        "\n",
        "def x_on_cpu(x):\n",
        "  return np.dot(x, x)\n",
        "\n",
        "%timeit -n 1 -r 1 x_on_cpu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbEk3aBC8hfc",
        "outputId": "fb263dd9-7a20-4316-a8c9-bd201185173c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 28.31 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "13.5 ms ± 22.9 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
          ]
        }
      ],
      "source": [
        "def x_on_gpu(x):\n",
        "  return jnp.dot(x, x)\n",
        "\n",
        "%timeit -n 5 -r 5 x_on_gpu(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpNIadOqJdL",
        "outputId": "c788668f-e10e-4da3-d952-7a919d5f545c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('MT19937', array([2147483648, 2308424580, 3659735007, 2320931750, 2404801303,\n",
            "       4077970435,  9 ...\n"
          ]
        }
      ],
      "source": [
        "def numpy_random_state():\n",
        "  print(str(np.random.get_state())[:100], '...')\n",
        "\n",
        "numpy_random_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2RFbBFQ9ewh"
      },
      "source": [
        "### Random numbers\n",
        "\n",
        "Generating random numbers can seem complicated at first glance.\n",
        "\n",
        "Pseudo random number generation (PRNG) creates sequences that aren't truly random because they're determined by their initial value, the `seed`. Each random sampling is a deterministic functino of a `state` carried between examples.\n",
        "\n",
        "In NumPy, PRNG is based on a global state, using the `numpy.random` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pte92wmv9pli",
        "outputId": "79fed67f-2a01-47d8-b679-60716865ae2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('MT19937', array([2147483648, 2308424580, 3659735007, 2320931750, 2404801303,\n",
            "       4077970435,  9 ...\n"
          ]
        }
      ],
      "source": [
        "def numpy_random_state():\n",
        "  print(str(np.random.get_state())[:100], '...')\n",
        "\n",
        "numpy_random_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7FJT9SOqiFT"
      },
      "source": [
        "This state is then updated by each call to `random`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97LAUP4eqnL_",
        "outputId": "b6b73ca0-ec57-4581-dfa9-2f54ede66248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('MT19937', array([         0,          1, 1812433255, 1900727105, 1208447044,\n",
            "       2481403966, 40 ...\n",
            "('MT19937', array([2443250962, 1093594115, 1878467924, 2709361018, 1101979660,\n",
            "       3904844661,  6 ...\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "numpy_random_state()\n",
        "\n",
        "_ = np.random.uniform()\n",
        "\n",
        "numpy_random_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlatgjmKqyAM"
      },
      "source": [
        "JAX handles PRNG differently since the framework intends to be easy to reproduce, parallelize and vectorize.\n",
        "\n",
        "Rather than use a global state, JAX uses a state called a `key`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNASrN0mrJPo",
        "outputId": "048079c7-1719-44ef-d9e7-87ccb9e027bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0 10]\n"
          ]
        }
      ],
      "source": [
        "key = random.PRNGKey(10)\n",
        "\n",
        "print(key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udEy-TmkrVCC"
      },
      "source": [
        "Random functions consume, and don't alter, the key. This means the same key should always produce the same sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLxjX3Zerfyb",
        "outputId": "a5dcb3a5-d5e7-4f3e-fc27-ce11442b8e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.3445405\n",
            "-1.3445405\n",
            "-1.3445405\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 3):\n",
        "  print(random.normal(key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGV4gOcNrwmr"
      },
      "source": [
        "One practice to bear in mind, then, is never to resuse keys, unless identical outputs are necessary. We can achieve independent keys by using the `split()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4SwyqRm2sIEI"
      },
      "outputs": [],
      "source": [
        "key, subkey = random.split(key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn-Cs7F7IcBI"
      },
      "source": [
        "### Intermediate representations\n",
        "\n",
        "Introducing the Jaxpr\n",
        "\n",
        "An *intermediate representation* is an internal interpretation of machine learning code used by underlying frameworks or compilers to optimize the program.\n",
        "When we write code in a framework such as JAX or PyTorch, it is converted from high-level code into a computational graph, or symbolic representation. This is further transformed into an intermediate representation (IR) optimized for efficiency.\n",
        "The IR is then optimized over several passes to conduct operations such as constant folding, operation fusion, model parallelization, and quantization. This is followed by hardware-specific compilation, to convert the IR into low-level code optimized for the required backend (TPU, GPU etc).\n",
        "\n",
        "The jaxpr\n",
        "\n",
        "JAX converts functions into an intermediate representation called a jaxpr . Transformations such as grad then work this the jaxpr representation.\n",
        "JAX works by tracing functions. Before we look at what that means, consider this simple Python function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5sMAmYP6Ix2K"
      },
      "outputs": [],
      "source": [
        "def sum_squares(x):\n",
        "    return jnp.sum(x**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St_OEa7II2GB"
      },
      "source": [
        "What does this function do? It looks easy at first glance, since it adds the result of x**2. x could be a single variable, or an array.\n",
        "However, given Python's dynamism, it could do anything depending on what `x` is...square an ice-cream order, print job or jackpot winnings in a slot machine.\n",
        "\n",
        "JAX takes advantage of this dynamism by running functions using tracer values. These are experimental inputs to a function, which help JAX understand how it works and what it will accomplish.\n",
        "\n",
        "We can take a look at the IR, the jaxpr, by using the `jax.make_jaxpr` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH9Bo7iQJV2I",
        "outputId": "0089f86a-d363-464f-dab3-b34b5a877b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ lambda ; a:f32[]. let\n",
            "    b:f32[] = integer_pow[y=2] a\n",
            "    c:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n",
            "    d:f32[] = reduce_sum[axes=()] c\n",
            "  in (d,) }\n"
          ]
        }
      ],
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "print(make_jaxpr(sum_squares)(3.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ram87J_J1J3"
      },
      "source": [
        "This result comprises the primitive operations, called lax, that JAX knows how to transform.\n",
        "\n",
        "Herein lies JAX's power; with no need to make the API complicated, JAX develops a sound idea of what the function is doing, and knows how to vectorize with `vmap`, parallelize with `pmap` , and how to just-in-time compile with `jit` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fqe9Ps0UVEp"
      },
      "source": [
        "## Transformations\n",
        "\n",
        "Modular, functional programming\n",
        "\n",
        "JAX can transform functions. This means a numerical function can be returned as a new function that, for example, computes the gradient of, or parallelizes the original function. It could also do both!\n",
        "\n",
        "### grad\n",
        "\n",
        "One of the most commonly used transformations, `jax.grad` calculates the gradient of a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KFQd1ns3UwTd"
      },
      "outputs": [],
      "source": [
        "from jax import grad\n",
        "\n",
        "def sum_squares(x):\n",
        "    return jnp.sum(x**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8AFXSPEUha"
      },
      "source": [
        "Since `jax.grad(f)` computes the gradient of function `f`, `jax.grad(f)(x)` is the gradient of f at x ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGyQ0nGNDrgm",
        "outputId": "fbf2ed42-0136-40ed-acbb-07cac73532dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ],
      "source": [
        "print(grad(sum_squares)(3.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTN7G9UIELva",
        "outputId": "c018d31c-f3c5-427f-be22-2b4c6ef25566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n"
          ]
        }
      ],
      "source": [
        "print(grad(grad(sum_squares))(3.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVNos55nuJh4",
        "outputId": "d10c9150-669e-4187-890e-5e8c755111e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.82300164692441\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def cylinder_volume(r, h):\n",
        "    vol = jnp.pi * r**2 * h\n",
        "    return vol\n",
        "\n",
        "# Compute the volume of a cylinder with radius 3, and height 3\n",
        "print(cylinder_volume(3, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjWiYPFFC3Fi",
        "outputId": "c15d1cf5-18bc-4b54-ae39-27a0476547e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201.06194\n",
            "75.398224\n"
          ]
        }
      ],
      "source": [
        "print(grad(cylinder_volume)(4.0, 8.0))\n",
        "print(grad(cylinder_volume)(2.0, 6.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyJWVzhCEBXt",
        "outputId": "e1aec796-b234-40ac-e90f-fe063b82c535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.265484\n",
            "37.699112\n"
          ]
        }
      ],
      "source": [
        "print(grad(grad(cylinder_volume))(4.0, 8.0))\n",
        "print(grad(grad(cylinder_volume))(2.0, 6.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTjbsZJ8UVHJ"
      },
      "source": [
        "We can use argnums to calculate the gradient with respect to different arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "03yxLLNODXuc"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  if x > 0:\n",
        "    return 2 * x ** 3\n",
        "  else:\n",
        "    return 3 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwbj3dAADVI9",
        "outputId": "ebc95133-98ba-4a99-d121-df836fb85c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0]\n",
            "-0.20584226\n",
            "3.0\n",
            "0.2542262\n"
          ]
        }
      ],
      "source": [
        "key = random.PRNGKey(0)\n",
        "x = random.normal(key, ())\n",
        "print(key)\n",
        "print(x)\n",
        "\n",
        "print(grad(f)(x))\n",
        "print(grad(f)(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6wLp0NMj6x"
      },
      "source": [
        "An obvious example to make use of `grad` would be a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LwSL8xoMqlR",
        "outputId": "a0641926-4703-4edc-c456-702879ba3031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.20000005 -0.19999981 -0.19999981 -0.19999981]\n"
          ]
        }
      ],
      "source": [
        "def loss(preds, targets):\n",
        "  return jnp.sum((preds-targets)**2)\n",
        "\n",
        "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
        "y = jnp.asarray([1.1, 2.1, 3.1, 4.1])\n",
        "\n",
        "print(grad(loss)(x, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAdNf7RdNS3F"
      },
      "source": [
        "### Value and grad\n",
        "\n",
        "We can return both the value and gradient of a function using `value_and_grad`. This is a common pattern in machine learning for logging training loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03p-sKpyNmx1",
        "outputId": "a73aa4a6-c311-40c7-98f0-e48c6550fa3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(0.03999995, dtype=float32),\n",
              " Array([-0.20000005, -0.19999981, -0.19999981, -0.19999981], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "value_and_grad(loss)(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTpDEBcRvBSH"
      },
      "source": [
        "### jit\n",
        "\n",
        "The `jax.jit()` transformation performs Just In Time (JIT) compilation of a JAX Python function for efficient execution in XLA.\n",
        "\n",
        "Let's go back to our `sum_squares()` function and time its original implementation on an array of numbers 1-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJj7euVju6Sd",
        "outputId": "aaf392b8-2c31-493f-e4b8-28337c161ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438 µs ± 33.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "from jax import jit\n",
        "\n",
        "def sum_squares(x):\n",
        "    return jnp.sum(x**2)\n",
        "\n",
        "x = jnp.arange(100)\n",
        "\n",
        "%timeit sum_squares(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvgsrfjvNWB"
      },
      "source": [
        "Let's jit the function and notice the speed improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJnVTxLu6VA",
        "outputId": "f203906f-e682-4603-f193-0e41e0d529b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65.2 µs ± 854 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "sum_squares_jit = jit(sum_squares)\n",
        "\n",
        "# Warm up\n",
        "sum_squares_jit(x).block_until_ready()\n",
        "\n",
        "%timeit sum_squares_jit(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltVor1StvTNh"
      },
      "source": [
        "In case this notation isn't familiar, µs denotes a 'microsecond', or a millionth of a second. ns is a 'nanosecond', a billionth of a second. Our jitted function is considerably faster in this simple example.\n",
        "Note: JAX's asynchronous execution model means the Python call might return before the computation ends. This is why we use the block_until_ready() method to make sure we return the end result.\n",
        "a returned array would not be populated as soon as the function returns. Using block_until_ready means we time the actual computation, not just the dispatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbe_Kt2nvVvY"
      },
      "source": [
        "### Sharp edges\n",
        "\n",
        "It isn't possible or economical to jit everything. jit will throw errors when function inputs spark conditional chains (eg if x < 5: ... ) and jit itself creates some overhead. jit is best reserved for compiling complex functions that will run several times, such as updating weights in a training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhfydR_kvcoh"
      },
      "source": [
        "### vmap\n",
        "\n",
        "The `jax.vmap` transformation generates a vectorized implementation of a function.\n",
        "\n",
        "[Reference](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#) for this section (thanks to DeepMind).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "a = jnp.ones(10)\n",
        "print(a)\n",
        "b = jnp.ones(10)\n",
        "print(b)\n",
        "\n",
        "make_jaxpr(jnp.dot)(a, b)"
      ],
      "metadata": {
        "id": "3KqfWjXdEWQL",
        "outputId": "030a81aa-5940-4ba5-df64-aa3b5caf943e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[10] b:f32[10]. let\n",
              "    c:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] a b\n",
              "  in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_jaxpr(vmap(jnp.dot))(a, b)"
      ],
      "metadata": {
        "id": "jBTPgcCwFUGW",
        "outputId": "698d11d4-fca1-4185-c39c-47980de21b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[10] b:f32[10]. let c:f32[10] = mul a b in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_a = jnp.ones((6, 4))\n",
        "print(matrix_a)\n",
        "matrix_b = jnp.ones((6, 4))\n",
        "print(matrix_b)\n",
        "\n",
        "make_jaxpr(vmap(jnp.dot))(matrix_a, matrix_b)"
      ],
      "metadata": {
        "id": "oG5ZcmE9EqCw",
        "outputId": "1b21d400-d190-4e5f-e2f4-d727d01ec785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n",
              "    c:f32[6] = dot_general[dimension_numbers=(([1], [1]), ([0], [0]))] a b\n",
              "  in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_jaxpr(vmap(vmap(jnp.dot)))(jnp.ones((10, 10, 8)), jnp.ones((10, 10, 8)))"
      ],
      "metadata": {
        "id": "Z7q-9dP-Fyr_",
        "outputId": "96c44612-1463-44f6-8d2c-c7baace47516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[10,10,8] b:f32[10,10,8]. let\n",
              "    c:f32[10,10] = dot_general[dimension_numbers=(([2], [2]), ([0, 1], [0, 1]))] a\n",
              "      b\n",
              "  in (c,) }"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5U18IewcF6"
      },
      "source": [
        "We can loop over a batch in Python however such operations tend to be costly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5ihPy46bvkjE"
      },
      "outputs": [],
      "source": [
        "from jax import vmap\n",
        "\n",
        "mat = random.normal(key, (150, 100))\n",
        "batched_x = random.normal(key, (10, 100))\n",
        "\n",
        "def apply_matrix(v):\n",
        "  return jnp.dot(mat, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNcmGRl6wlWK",
        "outputId": "bae9e2af-252e-4492-f091-d5881a1597bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naively batched\n",
            "3.76 ms ± 228 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "def naively_batched_apply_matrix(v_batched):\n",
        "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
        "\n",
        "print('Naively batched')\n",
        "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHiB_m4rwphR",
        "outputId": "f2bf8bc4-9a72-44f1-e6f5-64ea133b3fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "1.63 ms ± 386 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "def vmap_batched_apply_matrix(v_batched):\n",
        "  return vmap(apply_matrix)(v_batched)\n",
        "\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_jaxpr(vmap_batched_apply_matrix)(batched_x)"
      ],
      "metadata": {
        "id": "PICoS_oUD8-K",
        "outputId": "612ff7e4-3a77-41fa-9e7d-6c242d9dbe1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda a:f32[150,100]; b:f32[10,100]. let\n",
              "    c:f32[150,10] = dot_general[dimension_numbers=(([1], [1]), ([], []))] a b\n",
              "    d:f32[10,150] = transpose[permutation=(1, 0)] c\n",
              "  in (d,) }"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeJ3NGOqxijq",
        "outputId": "e6501a9f-e66f-49bc-8079-641169005fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jitted and auto-vectorized with vmap\n",
            "73.5 µs ± 1.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "@jit\n",
        "def jit_vmap_batched_apply_matrix(v_batched):\n",
        "  return vmap(apply_matrix)(v_batched)\n",
        "\n",
        "print('jitted and auto-vectorized with vmap')\n",
        "%timeit jit_vmap_batched_apply_matrix(batched_x).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_jaxpr(jit_vmap_batched_apply_matrix)(batched_x)"
      ],
      "metadata": {
        "id": "t4RhhTUhEFyc",
        "outputId": "c18027f8-5ce3-462f-b0d1-a07040a1db5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[10,100]. let\n",
              "    b:f32[10,150] = pjit[\n",
              "      jaxpr={ lambda c:f32[150,100]; d:f32[10,100]. let\n",
              "          e:f32[150,10] = dot_general[dimension_numbers=(([1], [1]), ([], []))] c\n",
              "            d\n",
              "          f:f32[10,150] = transpose[permutation=(1, 0)] e\n",
              "        in (f,) }\n",
              "      name=jit_vmap_batched_apply_matrix\n",
              "    ] a\n",
              "  in (b,) }"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YA0hFovcuJ"
      },
      "source": [
        "Putting it all together\n",
        "\n",
        "We take a loss function, use it to find gradients with grad, vectorize it for work across batches, then jit compile, all in one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iLnNPpVCFVOL"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, vmap, jit\n",
        "\n",
        "def predict(params, inputs):\n",
        "    for W, b in params:\n",
        "        outputs = jnp.dot(inputs, W) + b\n",
        "        inputs = jnp.tahn(outputs)\n",
        "    return outputs\n",
        "\n",
        "def mse_loss(params, batch):\n",
        "    inputs, targets = batch\n",
        "    preds = predict(params, inputs)\n",
        "    loss = jnp.sum((preds - targets) ** 2)\n",
        "    print(loss)\n",
        "    return loss\n",
        "\n",
        "gradients = jit(grad(mse_loss))\n",
        "vectorized_gradients = jit(vmap(grad(mse_loss), in_axes=(None, 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbDKrdb6vVx5"
      },
      "source": [
        "### pmap\n",
        "\n",
        "`pmap` expresses single-program, multiple-data (SPMD) computation. A `pmap` function will compile via XLA to execute in parallel, whether multiple GPUs or TPU cores.\n",
        "\n",
        "`vmap` vectorizes functions, `pmap` replicates functions and executes each replica on devices in parallel.\n",
        "\n",
        "`pmap` will require eight TPU cores to display its paralellization powers and won't work in a Colab at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1R2dBJ3Wf9D"
      },
      "outputs": [],
      "source": [
        "def pmap_batched_apply_matrix(v_batched):\n",
        "  return pmap(apply_matrix)(v_batched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5x38trpMREX"
      },
      "outputs": [],
      "source": [
        "pmap_batched_apply_matrix(batched_x)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (Local)",
      "language": "python",
      "name": "local-base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}