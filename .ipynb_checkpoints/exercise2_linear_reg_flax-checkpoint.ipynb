{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2iG/ZUvKsGUS/phPkkvLs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"z5lAxbrS145P"},"outputs":[],"source":[]},{"cell_type":"code","source":["import jax\n","from jax import numpy as jnp, random, lax, jit\n","from flax import linen as nn\n","\n","X = jnp.ones((1, 10))\n","Y = jnp.ones((5,))\n","\n","model = nn.Dense(features=5)\n","\n","@jit\n","def predict(params):\n","  return model.apply({'params': params}, X)\n","\n","@jit\n","def loss_fn(params):\n","  return jnp.mean(jnp.abs(Y - predict(params)))\n","\n","@jit\n","def init_params(rng):\n","  mlp_variables = model.init({'params': rng}, X)\n","  return mlp_variables['params']\n","\n","# Get initial parameters\n","params = init_params(jax.random.PRNGKey(42))\n","print(\"initial params\", params)\n","\n","# Run SGD.\n","for i in range(50):\n","  loss, grad = jax.value_and_grad(loss_fn)(params)\n","  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n","  lr = 0.03\n","  params = jax.tree_util.tree_map(lambda x, d: x - lr * d, params, grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaAZzJrN1_2p","executionInfo":{"status":"ok","timestamp":1685486924860,"user_tz":-60,"elapsed":3269,"user":{"displayName":"Robin Stringer","userId":"05796722230835218202"}},"outputId":"f36117e5-a194-49d6-cad4-bac232275010"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]},{"output_type":"stream","name":"stdout","text":["initial params FrozenDict({\n","    bias: Array([0., 0., 0., 0., 0.], dtype=float32),\n","    kernel: Array([[ 0.55015737,  0.41833436,  0.33977556,  0.43434998, -0.12176939],\n","           [ 0.07848787,  0.27258518, -0.22658114,  0.60018104,  0.27871728],\n","           [-0.3267914 , -0.45051524,  0.02986265, -0.5590184 ,  0.30982274],\n","           [ 0.05870081,  0.20131111, -0.15255067,  0.26707688, -0.6626963 ],\n","           [ 0.62493724, -0.20424645,  0.04849606,  0.25078458, -0.457508  ],\n","           [ 0.11445389,  0.08716885, -0.08621331,  0.42504248,  0.48199227],\n","           [ 0.01878028,  0.07163057, -0.21260886,  0.54276705, -0.1087756 ],\n","           [-0.08556435,  0.16303335, -0.54164785,  0.03657064, -0.0329951 ],\n","           [-0.10136051, -0.04724246,  0.20461115,  0.13906626, -0.04074767],\n","           [ 0.22522342,  0.32235065,  0.13873868,  0.13579997,  0.6603415 ]],      dtype=float32),\n","})\n","0 loss =  0.74939424 Yhat =  [[ 1.1570246   0.83440995 -0.45811766  2.2726204   0.30638173]]\n","1 loss =  0.6833943 Yhat =  [[ 1.0910246   0.90040994 -0.39211768  2.2066207   0.37238175]]\n","2 loss =  0.61739427 Yhat =  [[ 1.0250247   0.96641004 -0.3261178   2.1406205   0.43838173]]\n","3 loss =  0.58074844 Yhat =  [[ 0.9590247  1.0324101 -0.2601178  2.0746207  0.5043818]]\n","4 loss =  0.5381943 Yhat =  [[ 1.0250247   0.96641004 -0.19411783  2.0086207   0.5703817 ]]\n","5 loss =  0.5015484 Yhat =  [[ 0.9590247   1.0324101  -0.12811786  1.9426206   0.63638175]]\n","6 loss =  0.4589943 Yhat =  [[ 1.0250247   0.96641004 -0.06211786  1.8766208   0.70238173]]\n","7 loss =  0.4223485 Yhat =  [[0.9590247  1.0324101  0.00388213 1.8106207  0.76838166]]\n","8 loss =  0.37979433 Yhat =  [[1.0250247  0.96641004 0.06988217 1.7446208  0.83438164]]\n","9 loss =  0.34314847 Yhat =  [[0.9590247  1.0324101  0.13588214 1.6786208  0.9003817 ]]\n","10 loss =  0.3005943 Yhat =  [[1.0250247  0.96641004 0.20188223 1.6126206  0.9663816 ]]\n","11 loss =  0.2769011 Yhat =  [[0.9590247  1.0324101  0.26788223 1.5466207  1.0323817 ]]\n","12 loss =  0.24779435 Yhat =  [[1.0250247  0.96641004 0.3338822  1.4806209  0.9663816 ]]\n","13 loss =  0.22410117 Yhat =  [[0.9590247  1.0324101  0.39988223 1.4146209  1.0323817 ]]\n","14 loss =  0.19499432 Yhat =  [[1.0250247  0.96641004 0.4658823  1.3486209  0.9663816 ]]\n","15 loss =  0.17130111 Yhat =  [[0.9590247 1.0324101 0.5318823 1.2826208 1.0323817]]\n","16 loss =  0.14219432 Yhat =  [[1.0250247  0.96641004 0.5978823  1.2166208  0.9663816 ]]\n","17 loss =  0.11850115 Yhat =  [[0.9590247  1.0324101  0.66388226 1.1506209  1.0323817 ]]\n","18 loss =  0.089394264 Yhat =  [[1.0250247  0.96641004 0.7298824  1.0846207  0.9663816 ]]\n","19 loss =  0.06570112 Yhat =  [[0.9590247  1.0324101  0.79588234 1.0186208  1.0323817 ]]\n","20 loss =  0.055545952 Yhat =  [[1.0250247  0.96641004 0.86188245 0.9526208  0.9663816 ]]\n","21 loss =  0.039301097 Yhat =  [[0.9590247  1.0324101  0.92788243 1.0186208  1.0323817 ]]\n","22 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","23 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","24 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","25 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","26 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","27 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","28 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","29 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","30 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","31 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","32 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","33 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","34 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","35 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","36 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","37 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","38 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","39 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","40 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","41 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","42 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","43 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","44 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","45 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","46 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","47 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n","48 loss =  0.029145945 Yhat =  [[1.0250247  0.96641004 0.9938825  0.9526208  0.9663816 ]]\n","49 loss =  0.03685409 Yhat =  [[0.9590247 1.0324101 1.0598825 1.0186208 1.0323817]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RMdNpCkn2APn"},"execution_count":null,"outputs":[]}]}